# Backend Structure Documentation

## ğŸ“ Organized Backend Architecture

The backend has been reorganized from a flat structure into a well-organized, modular architecture following Python best practices.

### ğŸ¯ New Directory Structure

```
/app/backend/
â”œâ”€â”€ server.py                    # Main FastAPI application entry point
â”œâ”€â”€ .env                         # Environment variables
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ Dockerfile                   # Docker configuration
â”‚
â”œâ”€â”€ models/                      # ğŸ“Š Database Models
â”‚   â”œâ”€â”€ __init__.py             # Exports all models
â”‚   â”œâ”€â”€ user.py                 # User, UserCreate, UserLogin, UserResponse
â”‚   â”œâ”€â”€ actor.py                # Actor, ActorCreate, ActorUpdate, ActorPublish
â”‚   â”œâ”€â”€ run.py                  # Run, RunCreate, RunInput
â”‚   â”œâ”€â”€ dataset.py              # Dataset, DatasetItem
â”‚   â”œâ”€â”€ proxy.py                # Proxy, ProxyCreate
â”‚   â””â”€â”€ chat.py                 # LeadChatMessage, GlobalChatMessage, etc.
â”‚
â”œâ”€â”€ auth/                        # ğŸ” Authentication
â”‚   â”œâ”€â”€ __init__.py             # Exports auth functions
â”‚   â””â”€â”€ auth.py                 # JWT auth, password hashing
â”‚
â”œâ”€â”€ routes/                      # ğŸ›£ï¸ API Routes
â”‚   â”œâ”€â”€ __init__.py             # Exports router
â”‚   â””â”€â”€ routes.py               # All API endpoints
â”‚
â”œâ”€â”€ scrapers/                    # ğŸ•·ï¸ Scraping Engine & Scrapers
â”‚   â”œâ”€â”€ __init__.py             # Exports scraper classes
â”‚   â”œâ”€â”€ base_scraper.py         # BaseScraper abstract class
â”‚   â”œâ”€â”€ scraper_engine.py       # Playwright scraping engine
â”‚   â”œâ”€â”€ scraper_registry.py     # Dynamic scraper registration
â”‚   â”‚
â”‚   â”œâ”€â”€ googlemap/              # Google Maps Scraper
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ google_maps_scraper_v3.py
â”‚   â”‚
â”‚   â””â”€â”€ amazon/                 # Amazon Product Scraper
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ amazon_scraper.py
â”‚
â””â”€â”€ services/                    # ğŸ”§ Business Logic Services
    â”œâ”€â”€ __init__.py             # Exports service classes
    â”œâ”€â”€ proxy_manager.py        # Proxy rotation & health checks
    â”œâ”€â”€ task_manager.py         # Async task management
    â”œâ”€â”€ chat_service.py         # Lead AI chat service
    â””â”€â”€ global_chat_service_v2.py # Global AI assistant
```

## ğŸ”§ Import Patterns

### Before (Flat Structure)
```python
from models import User, Actor, Run
from auth import create_access_token
from scraper_engine import ScraperEngine
from google_maps_scraper_v3 import GoogleMapsScraperV3
from proxy_manager import get_proxy_manager
```

### After (Organized Structure)
```python
from models import User, Actor, Run
from auth import create_access_token
from scrapers import ScraperEngine, get_scraper_registry
from scrapers.googlemap import GoogleMapsScraperV3
from services import get_proxy_manager, get_task_manager
```

## ğŸ“¦ Package Exports

Each directory contains an `__init__.py` file that exports the relevant classes/functions:

### models/__init__.py
Exports all model classes: User, Actor, Run, Dataset, Proxy, Chat models

### auth/__init__.py
Exports: create_access_token, get_current_user, hash_password, verify_password

### scrapers/__init__.py
Exports: BaseScraper, ScraperEngine, ScraperRegistry, get_scraper_registry

### scrapers/googlemap/__init__.py
Exports: GoogleMapsScraperV3

### scrapers/amazon/__init__.py
Exports: AmazonProductScraper

### services/__init__.py
Exports: ProxyManager, TaskManager, LeadChatService, EnhancedGlobalChatService

### routes/__init__.py
Exports: router, set_db

## âœ… Benefits

1. **Clear Separation of Concerns**
   - Models separate from business logic
   - Scrapers grouped by type
   - Services isolated from routes

2. **Easy Navigation**
   - Find Google Maps scraper: `scrapers/googlemap/`
   - Find user models: `models/user.py`
   - Find auth logic: `auth/auth.py`

3. **Scalable Architecture**
   - Add new scrapers: Create `scrapers/linkedin/`
   - Add new models: Create `models/subscription.py`
   - Add new services: Create `services/email_service.py`

4. **Maintainable Codebase**
   - Smaller, focused files
   - Logical grouping
   - Easy to test individual components

5. **Professional Structure**
   - Follows Python package best practices
   - Similar to Django, Flask, FastAPI conventions
   - Industry-standard organization

## ğŸš€ Adding New Scrapers

To add a new scraper (e.g., LinkedIn):

1. Create directory: `scrapers/linkedin/`
2. Create `__init__.py`: Export the scraper class
3. Create `linkedin_scraper.py`: Implement scraper
4. Register in `scrapers/scraper_registry.py`: Add to auto_register_scrapers()

Example:
```python
# scrapers/linkedin/__init__.py
from .linkedin_scraper import LinkedInScraper
__all__ = ['LinkedInScraper']

# scrapers/scraper_registry.py - add to auto_register_scrapers()
from .linkedin.linkedin_scraper import LinkedInScraper
_global_registry.register(LinkedInScraper)
```

## ğŸ”„ Migration Notes

- Old flat structure files backed up in `_old_structure/`
- All imports updated throughout codebase
- Backward compatibility maintained
- API endpoints unchanged
- Frontend unaffected

## âœ… Verification

All services tested and working:
- âœ… Backend server running on port 8001
- âœ… Models importing correctly
- âœ… Auth system functional
- âœ… Scrapers registered and available
- âœ… Services (proxy, task, chat) operational
- âœ… API endpoints responding
- âœ… Database connections active

## ğŸ“š Related Files

- `server.py` - Main application entry point
- `requirements.txt` - Python dependencies
- `.env` - Environment configuration
- `Dockerfile` - Container setup
